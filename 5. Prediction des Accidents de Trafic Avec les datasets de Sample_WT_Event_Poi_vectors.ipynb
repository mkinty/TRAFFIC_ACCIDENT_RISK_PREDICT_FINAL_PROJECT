{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Importer des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /anaconda3/lib/python3.7/site-packages (2.0.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (1.16.4)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (0.1.8)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (0.33.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.15.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (41.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /anaconda3/lib/python3.7/site-packages (from rsa>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.7)\n",
      "Requirement already satisfied: requests>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: keras in /anaconda3/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /anaconda3/lib/python3.7/site-packages (from keras) (1.16.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /anaconda3/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /anaconda3/lib/python3.7/site-packages (from keras) (1.3.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda3/lib/python3.7/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /anaconda3/lib/python3.7/site-packages (from keras) (5.1.1)\n",
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.7/site-packages (from keras) (2.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Choix des Villes et les Intervalles de Coordonnées Géographiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = {'LosAngeles': [33.700615, 34.353627, -118.683511, -118.074559], \n",
    "           'Houston': [29.497907,30.129003,-95.797178,-94.988191],\n",
    "           'Austin': [30.079327, 30.596764,-97.968881,-97.504838],\n",
    "           'Dallas': [32.559567,33.083278,-97.036586,-96.428928],\n",
    "           'Charlotte': [34.970168,35.423667,-81.060925,-80.622687],\n",
    "           'Atlanta': [33.612410,33.916999,-84.575600,-84.231911]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Pre-processing de tous les datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "geohash_map = pd.read_csv(\"data/geohash_to_poi_vec.csv\")\n",
    "geo_dict = dict(zip(geohash_map.Geohash.unique(), range(len(geohash_map.Geohash.unique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filepath, cityname):\n",
    "    df = pd.read_csv(filepath)\n",
    "    print (\"Le taux de zero accident dans la ville de {} est égal à :\".format(cityname),float(df[df['T-Accident']==0].shape[0])/df.shape[0])\n",
    "    def fun_hash(geohash):\n",
    "        return geo_dict[geohash]\n",
    "    df['geohash_code'] = df.apply(lambda row: fun_hash(row['Geohash']), axis=1)\n",
    "    def week_day(DOW):\n",
    "        if DOW < 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def shift(group):\n",
    "        df_list=[]\n",
    "        for idx,df in group:\n",
    "            df['predicted_accident'] = df['T-Accident'].shift(-1)\n",
    "            df.drop(df.tail(1).index,inplace=True)\n",
    "            df_list.append(df)\n",
    "        return pd.concat(df_list)\n",
    "\n",
    "    def time_interval(HOD):\n",
    "        if HOD >=6 and HOD <10:\n",
    "            return 0\n",
    "        if HOD >= 10 and HOD<15:\n",
    "            return 1\n",
    "        if HOD >=15 and HOD< 18:\n",
    "            return 2;\n",
    "        if HOD >=18 and HOD< 22:\n",
    "            return 3\n",
    "        else:\n",
    "            return 4; \n",
    "    def make_binary(d):\n",
    "        if d > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0    \n",
    "    df['DOW'] = df.apply(lambda row: week_day(row['DOW']), axis=1)   \n",
    "    df['HOD'] = df.apply(lambda row: time_interval(row['HOD']), axis=1) \n",
    "    df['T-Accident'] = df.apply(lambda row: make_binary(row['T-Accident']), axis=1) \n",
    "    group = df.groupby('Geohash')\n",
    "    df = shift(group)\n",
    "    return df.to_csv(\"data/Clean_TW_Data/{}_Clean_TW_Data.csv\".format(cityname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LosAngeles = pd.read_csv(\"data/Clean_TW_Data/LosAngeles_Clean_TW_Data.csv\")\n",
    "Houston = pd.read_csv(\"data/Clean_TW_Data/Houston_Clean_TW_Data.csv\")\n",
    "Austin = pd.read_csv(\"data/Clean_TW_Data/Austin_Clean_TW_Data.csv\")\n",
    "Dallas = pd.read_csv(\"data/Clean_TW_Data/Dallas_Clean_TW_Data.csv\")\n",
    "Charlotte = pd.read_csv(\"data/Clean_TW_Data/Charlotte_Clean_TW_Data.csv\")\n",
    "Atlanta = pd.read_csv(\"data/Clean_TW_Data/Atlanta_Clean_TW_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geohash</th>\n",
       "      <th>TimeStep</th>\n",
       "      <th>DOW</th>\n",
       "      <th>HOD</th>\n",
       "      <th>DayLight</th>\n",
       "      <th>T-Accident</th>\n",
       "      <th>T-BrokenVehicle</th>\n",
       "      <th>T-Congestion</th>\n",
       "      <th>T-Construction</th>\n",
       "      <th>T-Event</th>\n",
       "      <th>...</th>\n",
       "      <th>Railway</th>\n",
       "      <th>Roundabout</th>\n",
       "      <th>Station</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Turning_Circle</th>\n",
       "      <th>Turning_Loop</th>\n",
       "      <th>geohash_code</th>\n",
       "      <th>predicted_accident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20574</th>\n",
       "      <td>9qh52</td>\n",
       "      <td>8122</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>307</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20575</th>\n",
       "      <td>9qh52</td>\n",
       "      <td>7611</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>307</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20576</th>\n",
       "      <td>9qh52</td>\n",
       "      <td>7734</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>307</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20577</th>\n",
       "      <td>9qh52</td>\n",
       "      <td>1197</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>307</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20578</th>\n",
       "      <td>9qh52</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>307</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Geohash  TimeStep  DOW  HOD  DayLight  T-Accident  T-BrokenVehicle  \\\n",
       "20574   9qh52      8122    1    1         1           0                0   \n",
       "20575   9qh52      7611    0    0         1           0                0   \n",
       "20576   9qh52      7734    1    1         1           0                0   \n",
       "20577   9qh52      1197    1    1         1           0                0   \n",
       "20578   9qh52        59    1    1         1           1                0   \n",
       "\n",
       "       T-Congestion  T-Construction  T-Event  ...  Railway  Roundabout  \\\n",
       "20574             0               0        0  ...        0           0   \n",
       "20575             0               0        0  ...        0           0   \n",
       "20576             0               0        0  ...        0           0   \n",
       "20577             0               0        0  ...        0           0   \n",
       "20578             0               0        0  ...        0           0   \n",
       "\n",
       "       Station  Stop  Traffic_Calming  Traffic_Signal  Turning_Circle  \\\n",
       "20574        0     0                0               0               0   \n",
       "20575        0     0                0               0               0   \n",
       "20576        0     0                0               0               0   \n",
       "20577        0     0                0               0               0   \n",
       "20578        0     0                0               0               0   \n",
       "\n",
       "       Turning_Loop  geohash_code  predicted_accident  \n",
       "20574             0           307                 0.0  \n",
       "20575             0           307                 0.0  \n",
       "20576             0           307                 0.0  \n",
       "20577             0           307                 1.0  \n",
       "20578             0           307                 1.0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LosAngeles.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le taux de zero accident dans la ville de LosAngeles est égal à : 0.6269592476489029\n",
      "Le taux de zero accident dans la ville de Houston est égal à : 0.6895820072140887\n",
      "Le taux de zero accident dans la ville de Austin est égal à : 0.752924823352253\n",
      "Le taux de zero accident dans la ville de Dallas est égal à : 0.7944267905157664\n",
      "Le taux de zero accident dans la ville de Charlotte est égal à : 0.7106155023504974\n",
      "Le taux de zero accident dans la ville de Atlanta est égal à : 0.8316274309109519\n"
     ]
    }
   ],
   "source": [
    "for city in cities:\n",
    "    clean_data(\"data/Sample_TW_Event_Poi_vectors/Sample_Geohash_{}_TW_Event_Poi.csv\".format(city), city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1- Concaténation des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in cities :\n",
    "    df = pd.read_csv(\"data/Clean_TW_Data/{}_Clean_TW_Data.csv\".format(city))\n",
    "    df = df.drop([\"Geohash\", \"T-Accident\",\"T-BrokenVehicle\",\"T-Congestion\",\"T-Construction\",\"T-Event\", \"T-FlowIncident\", \"T-RoadBlocked\", \"T-Other\"], axis = 1)\n",
    "    df.to_csv(\"data/clean_twpoi_data/{}.csv\".format(city), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/clean_twpoi_data/Atlanta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeStep</th>\n",
       "      <th>DOW</th>\n",
       "      <th>HOD</th>\n",
       "      <th>DayLight</th>\n",
       "      <th>W-Humidity</th>\n",
       "      <th>W-Precipitation</th>\n",
       "      <th>W-Pressure</th>\n",
       "      <th>W-Temperature</th>\n",
       "      <th>W-Visibility</th>\n",
       "      <th>W-WindSpeed</th>\n",
       "      <th>...</th>\n",
       "      <th>Railway</th>\n",
       "      <th>Roundabout</th>\n",
       "      <th>Station</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Turning_Circle</th>\n",
       "      <th>Turning_Loop</th>\n",
       "      <th>geohash_code</th>\n",
       "      <th>predicted_accident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15559</th>\n",
       "      <td>8206</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15560</th>\n",
       "      <td>8529</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15561</th>\n",
       "      <td>7149</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15562</th>\n",
       "      <td>1126</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15563</th>\n",
       "      <td>4500</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TimeStep  DOW  HOD  DayLight  W-Humidity  W-Precipitation  W-Pressure  \\\n",
       "15559      8206    0    1         1           0                0           0   \n",
       "15560      8529    1    3         0           0                0           0   \n",
       "15561      7149    1    1         1           0                0           0   \n",
       "15562      1126    1    2         1           0                0           0   \n",
       "15563      4500    1    3         0           0                0           0   \n",
       "\n",
       "       W-Temperature  W-Visibility  W-WindSpeed  ...  Railway  Roundabout  \\\n",
       "15559              0             0            0  ...       20           0   \n",
       "15560              0             0            0  ...       20           0   \n",
       "15561              0             0            0  ...       20           0   \n",
       "15562              0             0            0  ...       20           0   \n",
       "15563              0             0            0  ...       20           0   \n",
       "\n",
       "       Station  Stop  Traffic_Calming  Traffic_Signal  Turning_Circle  \\\n",
       "15559        0     0                8               5              71   \n",
       "15560        0     0                8               5              71   \n",
       "15561        0     0                8               5              71   \n",
       "15562        0     0                8               5              71   \n",
       "15563        0     0                8               5              71   \n",
       "\n",
       "       Turning_Loop  geohash_code  predicted_accident  \n",
       "15559             0           489                 0.0  \n",
       "15560             0           489                 0.0  \n",
       "15561             0           489                 1.0  \n",
       "15562             0           489                 1.0  \n",
       "15563             0           489                 1.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([LosAngeles, Houston, Austin, Dallas, Charlotte, Atlanta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"data/clean_twpoi_data/clean_twpoi_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/clean_twpoi_data/clean_twpoi_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le taux de zero accident  est égal à : 0.7257439159812539\n"
     ]
    }
   ],
   "source": [
    "print (\"Le taux de zero accident  est égal à :\" ,float(df[df['predicted_accident']==0].shape[0])/df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeStep</th>\n",
       "      <th>DOW</th>\n",
       "      <th>HOD</th>\n",
       "      <th>DayLight</th>\n",
       "      <th>W-Humidity</th>\n",
       "      <th>W-Precipitation</th>\n",
       "      <th>W-Pressure</th>\n",
       "      <th>W-Temperature</th>\n",
       "      <th>W-Visibility</th>\n",
       "      <th>W-WindSpeed</th>\n",
       "      <th>...</th>\n",
       "      <th>Railway</th>\n",
       "      <th>Roundabout</th>\n",
       "      <th>Station</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Turning_Circle</th>\n",
       "      <th>Turning_Loop</th>\n",
       "      <th>geohash_code</th>\n",
       "      <th>predicted_accident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106257</th>\n",
       "      <td>8206</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106258</th>\n",
       "      <td>8529</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106259</th>\n",
       "      <td>7149</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106260</th>\n",
       "      <td>1126</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106261</th>\n",
       "      <td>4500</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TimeStep  DOW  HOD  DayLight  W-Humidity  W-Precipitation  W-Pressure  \\\n",
       "106257      8206    0    1         1           0                0           0   \n",
       "106258      8529    1    3         0           0                0           0   \n",
       "106259      7149    1    1         1           0                0           0   \n",
       "106260      1126    1    2         1           0                0           0   \n",
       "106261      4500    1    3         0           0                0           0   \n",
       "\n",
       "        W-Temperature  W-Visibility  W-WindSpeed  ...  Railway  Roundabout  \\\n",
       "106257              0             0            0  ...       20           0   \n",
       "106258              0             0            0  ...       20           0   \n",
       "106259              0             0            0  ...       20           0   \n",
       "106260              0             0            0  ...       20           0   \n",
       "106261              0             0            0  ...       20           0   \n",
       "\n",
       "        Station  Stop  Traffic_Calming  Traffic_Signal  Turning_Circle  \\\n",
       "106257        0     0                8               5              71   \n",
       "106258        0     0                8               5              71   \n",
       "106259        0     0                8               5              71   \n",
       "106260        0     0                8               5              71   \n",
       "106261        0     0                8               5              71   \n",
       "\n",
       "        Turning_Loop  geohash_code  predicted_accident  \n",
       "106257             0           489                 0.0  \n",
       "106258             0           489                 0.0  \n",
       "106259             0           489                 1.0  \n",
       "106260             0           489                 1.0  \n",
       "106261             0           489                 1.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/clean_twpoi_data/TrafficWeatherEvent_June18_Aug18_Publish.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2- Importation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeStep</th>\n",
       "      <th>DOW</th>\n",
       "      <th>HOD</th>\n",
       "      <th>DayLight</th>\n",
       "      <th>W-Humidity</th>\n",
       "      <th>W-Precipitation</th>\n",
       "      <th>W-Pressure</th>\n",
       "      <th>W-Temperature</th>\n",
       "      <th>W-Visibility</th>\n",
       "      <th>W-WindSpeed</th>\n",
       "      <th>...</th>\n",
       "      <th>Railway</th>\n",
       "      <th>Roundabout</th>\n",
       "      <th>Station</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Turning_Circle</th>\n",
       "      <th>Turning_Loop</th>\n",
       "      <th>geohash_code</th>\n",
       "      <th>predicted_accident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106257</th>\n",
       "      <td>8206</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106258</th>\n",
       "      <td>8529</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106259</th>\n",
       "      <td>7149</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106260</th>\n",
       "      <td>1126</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106261</th>\n",
       "      <td>4500</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TimeStep  DOW  HOD  DayLight  W-Humidity  W-Precipitation  W-Pressure  \\\n",
       "106257      8206    0    1         1           0                0           0   \n",
       "106258      8529    1    3         0           0                0           0   \n",
       "106259      7149    1    1         1           0                0           0   \n",
       "106260      1126    1    2         1           0                0           0   \n",
       "106261      4500    1    3         0           0                0           0   \n",
       "\n",
       "        W-Temperature  W-Visibility  W-WindSpeed  ...  Railway  Roundabout  \\\n",
       "106257              0             0            0  ...       20           0   \n",
       "106258              0             0            0  ...       20           0   \n",
       "106259              0             0            0  ...       20           0   \n",
       "106260              0             0            0  ...       20           0   \n",
       "106261              0             0            0  ...       20           0   \n",
       "\n",
       "        Station  Stop  Traffic_Calming  Traffic_Signal  Turning_Circle  \\\n",
       "106257        0     0                8               5              71   \n",
       "106258        0     0                8               5              71   \n",
       "106259        0     0                8               5              71   \n",
       "106260        0     0                8               5              71   \n",
       "106261        0     0                8               5              71   \n",
       "\n",
       "        Turning_Loop  geohash_code  predicted_accident  \n",
       "106257             0           489                 0.0  \n",
       "106258             0           489                 0.0  \n",
       "106259             0           489                 1.0  \n",
       "106260             0           489                 1.0  \n",
       "106261             0           489                 1.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw = pd.read_csv(\"data/clean_twpoi_data/TrafficWeatherEvent_June18_Aug18_Publish.csv\")\n",
    "tw.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Models de prédictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-1- Modèle de Regression Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(filepath, cityname):\n",
    "    df = pd.read_csv(filepath)\n",
    "    X = df.loc[:, df.columns != \"predicted_accident\"]\n",
    "    y = df.loc[:, df.columns == \"predicted_accident\"]\n",
    "    \n",
    "    # Split in train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 0)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Standardisize\n",
    "    sc_x = StandardScaler()\n",
    "    X_train = sc_x.fit_transform(X_train)\n",
    "    X_test = sc_x.transform(X_test)\n",
    "    \n",
    "    # Modele de regression Logistique\n",
    "    print(\"{},  logistic regression  ... \".format(cityname))\n",
    "    classifier = LogisticRegression( )\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print()\n",
    "    print(\"{}, score de performance du modèle (train) : {}  \".format(cityname, classifier.score(X_train, y_train)))\n",
    "    print(\"{}, score de performance du modèle (test) : {}  \".format(cityname, classifier.score(X_test, y_test)))\n",
    "    print()\n",
    "    predictions = classifier.predict(X_test)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print()\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"....Done\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = {\"TrafficWeatherEvent_June18_Aug18_Publish\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrafficWeatherEvent_June18_Aug18_Publish,  logistic regression  ... \n",
      "\n",
      "TrafficWeatherEvent_June18_Aug18_Publish, score de performance du modèle (train) : 0.7850580526767754  \n",
      "TrafficWeatherEvent_June18_Aug18_Publish, score de performance du modèle (test) : 0.780501576248059  \n",
      "\n",
      "Confusion Matrix:\n",
      "[[14326  1098]\n",
      " [ 3567  2262]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.93      0.86     15424\n",
      "         1.0       0.67      0.39      0.49      5829\n",
      "\n",
      "    accuracy                           0.78     21253\n",
      "   macro avg       0.74      0.66      0.68     21253\n",
      "weighted avg       0.77      0.78      0.76     21253\n",
      "\n",
      "....Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for city in c :\n",
    "    logistic_regression(\"data/clean_twpoi_data/{}.csv\".format(city), city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LosAngeles,  logistic regression  ... \n",
      "\n",
      "LosAngeles, score de performance du modèle (train) : 0.7235619267448217  \n",
      "LosAngeles, score de performance du modèle (test) : 0.7074829931972789  \n",
      "\n",
      "Confusion Matrix:\n",
      "[[2192  377]\n",
      " [ 827  720]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.85      0.78      2569\n",
      "         1.0       0.66      0.47      0.54      1547\n",
      "\n",
      "    accuracy                           0.71      4116\n",
      "   macro avg       0.69      0.66      0.66      4116\n",
      "weighted avg       0.70      0.71      0.69      4116\n",
      "\n",
      "....Done\n",
      "\n",
      "Houston,  logistic regression  ... \n",
      "\n",
      "Houston, score de performance du modèle (train) : 0.8166913049321328  \n",
      "Houston, score de performance du modèle (test) : 0.8132222520827734  \n",
      "\n",
      "Confusion Matrix:\n",
      "[[2313  237]\n",
      " [ 458  713]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.91      0.87      2550\n",
      "         1.0       0.75      0.61      0.67      1171\n",
      "\n",
      "    accuracy                           0.81      3721\n",
      "   macro avg       0.79      0.76      0.77      3721\n",
      "weighted avg       0.81      0.81      0.81      3721\n",
      "\n",
      "....Done\n",
      "\n",
      "Austin,  logistic regression  ... \n",
      "\n",
      "Austin, score de performance du modèle (train) : 0.8374671724540415  \n",
      "Austin, score de performance du modèle (test) : 0.8415985997666278  \n",
      "\n",
      "Confusion Matrix:\n",
      "[[2382  193]\n",
      " [ 350  503]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.93      0.90      2575\n",
      "         1.0       0.72      0.59      0.65       853\n",
      "\n",
      "    accuracy                           0.84      3428\n",
      "   macro avg       0.80      0.76      0.77      3428\n",
      "weighted avg       0.83      0.84      0.84      3428\n",
      "\n",
      "....Done\n",
      "\n",
      "Dallas,  logistic regression  ... \n",
      "\n",
      "Dallas, score de performance du modèle (train) : 0.8243055555555555  \n",
      "Dallas, score de performance du modèle (test) : 0.8277777777777777  \n",
      "\n",
      "Confusion Matrix:\n",
      "[[2460  107]\n",
      " [ 451  222]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.96      0.90      2567\n",
      "         1.0       0.67      0.33      0.44       673\n",
      "\n",
      "    accuracy                           0.83      3240\n",
      "   macro avg       0.76      0.64      0.67      3240\n",
      "weighted avg       0.81      0.83      0.80      3240\n",
      "\n",
      "....Done\n",
      "\n",
      "Charlotte,  logistic regression  ... \n",
      "\n",
      "Charlotte, score de performance du modèle (train) : 0.7948294829482948  \n",
      "Charlotte, score de performance du modèle (test) : 0.7871287128712872  \n",
      "\n",
      "Confusion Matrix:\n",
      "[[2388  189]\n",
      " [ 585  474]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.93      0.86      2577\n",
      "         1.0       0.71      0.45      0.55      1059\n",
      "\n",
      "    accuracy                           0.79      3636\n",
      "   macro avg       0.76      0.69      0.71      3636\n",
      "weighted avg       0.78      0.79      0.77      3636\n",
      "\n",
      "....Done\n",
      "\n",
      "Atlanta,  logistic regression  ... \n",
      "\n",
      "Atlanta, score de performance du modèle (train) : 0.8674002088185688  \n",
      "Atlanta, score de performance du modèle (test) : 0.8740764535817539  \n",
      "\n",
      "Confusion Matrix:\n",
      "[[2492   95]\n",
      " [ 297  229]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.96      0.93      2587\n",
      "         1.0       0.71      0.44      0.54       526\n",
      "\n",
      "    accuracy                           0.87      3113\n",
      "   macro avg       0.80      0.70      0.73      3113\n",
      "weighted avg       0.86      0.87      0.86      3113\n",
      "\n",
      "....Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for city in cities :\n",
    "    logistic_regression(\"data/clean_twpoi_data/{}.csv\".format(city), city)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2- Modèle de Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting(filepath, cityname):\n",
    "    df = pd.read_csv(filepath)\n",
    "    X = df.loc[:, df.columns != \"predicted_accident\"]\n",
    "    y = df.loc[:, df.columns == \"predicted_accident\"]\n",
    "    \n",
    "    # Split in train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = 42)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Standardisize\n",
    "    sc_x = StandardScaler()\n",
    "    X_train = sc_x.fit_transform(X_train)\n",
    "    X_test = sc_x.transform(X_test)\n",
    "    \n",
    "    # Gradient Boosting Classifier\n",
    "    print(\"{},  Gradient Boosting Classifier  ... \".format(cityname))\n",
    "   \n",
    "    gb_clf2 = GradientBoostingClassifier(n_estimators = 90, learning_rate=0.95, \n",
    "                                        random_state=0)\n",
    "    gb_clf2.fit(X_train, y_train)\n",
    "    print()\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf2.score(X_train, y_train)))\n",
    "    print(\"Accuracy score (test): {0:.3f}\".format(gb_clf2.score(X_test, y_test)))\n",
    "    print()\n",
    "    \n",
    "    predictions = gb_clf2.predict(X_test)\n",
    "\n",
    "    print()\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    \n",
    "    print()\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"....Done\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrafficWeatherEvent_June18_Aug18_Publish,  Gradient Boosting Classifier  ... \n",
      "\n",
      "Accuracy score (training): 0.833\n",
      "Accuracy score (test): 0.827\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[20665  2471]\n",
      " [ 3058  5685]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.89      0.88     23136\n",
      "         1.0       0.70      0.65      0.67      8743\n",
      "\n",
      "    accuracy                           0.83     31879\n",
      "   macro avg       0.78      0.77      0.78     31879\n",
      "weighted avg       0.82      0.83      0.82     31879\n",
      "\n",
      "....Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for city in c :\n",
    "    gradient_boosting(\"data/clean_twpoi_data/{}.csv\".format(city), city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting(filepath, cityname):\n",
    "    df = pd.read_csv(filepath)\n",
    "    X = df.loc[:, df.columns != \"predicted_accident\"]\n",
    "    y = df.loc[:, df.columns == \"predicted_accident\"]\n",
    "    \n",
    "    # Split in train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = 42)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Standardisize\n",
    "    sc_x = StandardScaler()\n",
    "    X_train = sc_x.fit_transform(X_train)\n",
    "    X_test = sc_x.transform(X_test)\n",
    "    \n",
    "    # Gradient Boosting Classifier\n",
    "    print(\"{},  Gradient Boosting Classifier  ... \".format(cityname))\n",
    "    parameters = {'n_estimators':[20, 30, 40, 50, 70, 100, 150, 200],\n",
    "                  \"learning_rate\":[1.0, 0.9, 0.8, 0.7, 0.6, 0.5]}\n",
    "    gb_clf = GradientBoostingClassifier()\n",
    "    gb_clf_best = GridSearchCV(gb_clf, parameters)\n",
    "    gb_clf_best.fit(X_train, y_train)\n",
    "    print()\n",
    "    print(\"Best parameters: \", gb_clf_best.best_params_)\n",
    "    print()\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf_best.best_estimator_.score(X_train, y_train)))\n",
    "    print(\"Accuracy score (test): {0:.3f}\".format(gb_clf_best.best_estimator_.score(X_test, y_test)))\n",
    "    print()\n",
    "    best_params = gb_clf_best.best_params_\n",
    "    n_estimators = best_params['n_estimators']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "    gb_clf2 = GradientBoostingClassifier(n_estimators = n_estimators, learning_rate=learning_rate, \n",
    "                                         random_state=0)\n",
    "    gb_clf2.fit(X_train, y_train)\n",
    "    predictions = gb_clf2.predict(X_test)\n",
    "\n",
    "    print()\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    \n",
    "    print()\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"....Done\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LosAngeles,  Gradient Boosting Classifier  ... \n",
      "\n",
      "Best parameters:  {'learning_rate': 0.5, 'n_estimators': 20}\n",
      "\n",
      "Accuracy score (training): 0.756\n",
      "Accuracy score (test): 0.751\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3087  766]\n",
      " [ 769 1552]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.80      0.80      3853\n",
      "         1.0       0.67      0.67      0.67      2321\n",
      "\n",
      "    accuracy                           0.75      6174\n",
      "   macro avg       0.74      0.73      0.73      6174\n",
      "weighted avg       0.75      0.75      0.75      6174\n",
      "\n",
      "....Done\n",
      "\n",
      "Houston,  Gradient Boosting Classifier  ... \n",
      "\n",
      "Best parameters:  {'learning_rate': 0.7, 'n_estimators': 20}\n",
      "\n",
      "Accuracy score (training): 0.842\n",
      "Accuracy score (test): 0.838\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3421  404]\n",
      " [ 498 1258]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.89      0.88      3825\n",
      "         1.0       0.76      0.72      0.74      1756\n",
      "\n",
      "    accuracy                           0.84      5581\n",
      "   macro avg       0.81      0.81      0.81      5581\n",
      "weighted avg       0.84      0.84      0.84      5581\n",
      "\n",
      "....Done\n",
      "\n",
      "Austin,  Gradient Boosting Classifier  ... \n",
      "\n",
      "Best parameters:  {'learning_rate': 0.5, 'n_estimators': 40}\n",
      "\n",
      "Accuracy score (training): 0.869\n",
      "Accuracy score (test): 0.846\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3523  338]\n",
      " [ 453  827]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.91      0.90      3861\n",
      "         1.0       0.71      0.65      0.68      1280\n",
      "\n",
      "    accuracy                           0.85      5141\n",
      "   macro avg       0.80      0.78      0.79      5141\n",
      "weighted avg       0.84      0.85      0.84      5141\n",
      "\n",
      "....Done\n",
      "\n",
      "Dallas,  Gradient Boosting Classifier  ... \n",
      "\n",
      "Best parameters:  {'learning_rate': 0.7, 'n_estimators': 50}\n",
      "\n",
      "Accuracy score (training): 0.863\n",
      "Accuracy score (test): 0.845\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3595  256]\n",
      " [ 499  510]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.93      0.90      3851\n",
      "         1.0       0.67      0.51      0.57      1009\n",
      "\n",
      "    accuracy                           0.84      4860\n",
      "   macro avg       0.77      0.72      0.74      4860\n",
      "weighted avg       0.83      0.84      0.84      4860\n",
      "\n",
      "....Done\n",
      "\n",
      "Charlotte,  Gradient Boosting Classifier  ... \n",
      "\n",
      "Best parameters:  {'learning_rate': 0.5, 'n_estimators': 20}\n",
      "\n",
      "Accuracy score (training): 0.834\n",
      "Accuracy score (test): 0.822\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3444  422]\n",
      " [ 547 1041]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.89      0.88      3866\n",
      "         1.0       0.71      0.66      0.68      1588\n",
      "\n",
      "    accuracy                           0.82      5454\n",
      "   macro avg       0.79      0.77      0.78      5454\n",
      "weighted avg       0.82      0.82      0.82      5454\n",
      "\n",
      "....Done\n",
      "\n",
      "Atlanta,  Gradient Boosting Classifier  ... \n",
      "\n",
      "Best parameters:  {'learning_rate': 0.6, 'n_estimators': 20}\n",
      "\n",
      "Accuracy score (training): 0.889\n",
      "Accuracy score (test): 0.877\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3677  203]\n",
      " [ 371  419]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.95      0.93      3880\n",
      "         1.0       0.67      0.53      0.59       790\n",
      "\n",
      "    accuracy                           0.88      4670\n",
      "   macro avg       0.79      0.74      0.76      4670\n",
      "weighted avg       0.87      0.88      0.87      4670\n",
      "\n",
      "....Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for city in cities :\n",
    "    gradient_boosting(\"data/clean_twpoi_data/{}.csv\".format(city), city)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ann(filepath, cityname) :\n",
    "    df = pd.read_csv(filepath)\n",
    "    X = df.loc[:, df.columns != \"predicted_accident\"]\n",
    "    y = df.loc[:, df.columns == \"predicted_accident\"]\n",
    "    y = y.values\n",
    "    # Split in train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 0)\n",
    "\n",
    "    # Standardisize\n",
    "    sc_x = StandardScaler()\n",
    "    X_train = sc_x.fit_transform(X_train)\n",
    "    X_test = sc_x.transform(X_test)\n",
    "    # Modele ANN\n",
    "    print(\"{},  Artifical Neural Network (ANN) train ... \".format(cityname))\n",
    "    print()\n",
    "    classifier = keras.Sequential([\n",
    "                        keras.layers.Dense(512, activation = tf.nn.relu, input_dim = 30),\n",
    "                        keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "                        #keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                        keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "                    ])\n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Fitting the ANN to the Training set\n",
    "    classifier.fit(X_train, y_train,  batch_size = 10, validation_split= 0.2,  epochs = 10)\n",
    "    print(\"....Done\")\n",
    "    print()\n",
    "    print(\"{},  Artifical Neural Network (ANN) Score test  ... \".format(cityname))\n",
    "    classifier.evaluate(X_test, y_test)\n",
    "    predictions = classifier.predict(X_test)\n",
    "    predictions = (predictions>0.5)\n",
    "    print(\"....Done\")\n",
    "    print()\n",
    "    print(\"{},  Artifical Neural Network (ANN) Confusion Matrix et Classification Report  ... \".format(cityname))\n",
    "    print()\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print()\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"....Done\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LosAngeles,  Artifical Neural Network (ANN) train ... \n",
      "\n",
      "Train on 13170 samples, validate on 3293 samples\n",
      "Epoch 1/10\n",
      "13170/13170 [==============================] - 6s 448us/step - loss: 0.5237 - accuracy: 0.7260 - val_loss: 0.4948 - val_accuracy: 0.7382\n",
      "Epoch 2/10\n",
      "13170/13170 [==============================] - 6s 492us/step - loss: 0.5057 - accuracy: 0.7356 - val_loss: 0.5182 - val_accuracy: 0.7379\n",
      "Epoch 3/10\n",
      "13170/13170 [==============================] - 5s 384us/step - loss: 0.4988 - accuracy: 0.7421 - val_loss: 0.5012 - val_accuracy: 0.7337\n",
      "Epoch 4/10\n",
      "13170/13170 [==============================] - 5s 367us/step - loss: 0.4939 - accuracy: 0.7438 - val_loss: 0.4830 - val_accuracy: 0.7355\n",
      "Epoch 5/10\n",
      "13170/13170 [==============================] - 6s 440us/step - loss: 0.4912 - accuracy: 0.7424 - val_loss: 0.4944 - val_accuracy: 0.7428\n",
      "Epoch 6/10\n",
      "13170/13170 [==============================] - 6s 467us/step - loss: 0.4889 - accuracy: 0.7462 - val_loss: 0.5010 - val_accuracy: 0.7364\n",
      "Epoch 7/10\n",
      "13170/13170 [==============================] - 6s 444us/step - loss: 0.4854 - accuracy: 0.7468 - val_loss: 0.4874 - val_accuracy: 0.7388\n",
      "Epoch 8/10\n",
      "13170/13170 [==============================] - 6s 462us/step - loss: 0.4834 - accuracy: 0.7496 - val_loss: 0.4823 - val_accuracy: 0.7455\n",
      "Epoch 9/10\n",
      "13170/13170 [==============================] - 7s 544us/step - loss: 0.4810 - accuracy: 0.7479 - val_loss: 0.4787 - val_accuracy: 0.7449\n",
      "Epoch 10/10\n",
      "13170/13170 [==============================] - 7s 538us/step - loss: 0.4769 - accuracy: 0.7552 - val_loss: 0.4938 - val_accuracy: 0.7437\n",
      "....Done\n",
      "\n",
      "LosAngeles,  Artifical Neural Network (ANN) Score test  ... \n",
      "4116/4116 [==============================] - 0s 55us/step\n",
      "....Done\n",
      "\n",
      "LosAngeles,  Artifical Neural Network (ANN) Confusion Matrix et Classification Report  ... \n",
      "\n",
      "Confusion Matrix:\n",
      "[[2089  480]\n",
      " [ 621  926]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.81      0.79      2569\n",
      "         1.0       0.66      0.60      0.63      1547\n",
      "\n",
      "    accuracy                           0.73      4116\n",
      "   macro avg       0.71      0.71      0.71      4116\n",
      "weighted avg       0.73      0.73      0.73      4116\n",
      "\n",
      "....Done\n",
      "\n",
      "Houston,  Artifical Neural Network (ANN) train ... \n",
      "\n",
      "Train on 11905 samples, validate on 2977 samples\n",
      "Epoch 1/10\n",
      "11905/11905 [==============================] - 8s 639us/step - loss: 0.4061 - accuracy: 0.8133 - val_loss: 0.3875 - val_accuracy: 0.8310\n",
      "Epoch 2/10\n",
      "11905/11905 [==============================] - 6s 540us/step - loss: 0.3881 - accuracy: 0.8263 - val_loss: 0.3751 - val_accuracy: 0.8351\n",
      "Epoch 3/10\n",
      "11905/11905 [==============================] - 6s 543us/step - loss: 0.3814 - accuracy: 0.8279 - val_loss: 0.3747 - val_accuracy: 0.8357\n",
      "Epoch 4/10\n",
      "11905/11905 [==============================] - 7s 554us/step - loss: 0.3771 - accuracy: 0.8297 - val_loss: 0.3733 - val_accuracy: 0.8394\n",
      "Epoch 5/10\n",
      "11905/11905 [==============================] - 6s 529us/step - loss: 0.3746 - accuracy: 0.8317 - val_loss: 0.3675 - val_accuracy: 0.8398\n",
      "Epoch 6/10\n",
      "11905/11905 [==============================] - 6s 524us/step - loss: 0.3708 - accuracy: 0.8325 - val_loss: 0.3664 - val_accuracy: 0.8394\n",
      "Epoch 7/10\n",
      "11905/11905 [==============================] - 7s 553us/step - loss: 0.3682 - accuracy: 0.8334 - val_loss: 0.3710 - val_accuracy: 0.8421\n",
      "Epoch 8/10\n",
      "11905/11905 [==============================] - 7s 555us/step - loss: 0.3657 - accuracy: 0.8349 - val_loss: 0.3719 - val_accuracy: 0.8398\n",
      "Epoch 9/10\n",
      "11905/11905 [==============================] - 6s 489us/step - loss: 0.3631 - accuracy: 0.8375 - val_loss: 0.3705 - val_accuracy: 0.8441\n",
      "Epoch 10/10\n",
      "11905/11905 [==============================] - 5s 459us/step - loss: 0.3611 - accuracy: 0.8370 - val_loss: 0.3706 - val_accuracy: 0.8428\n",
      "....Done\n",
      "\n",
      "Houston,  Artifical Neural Network (ANN) Score test  ... \n",
      "3721/3721 [==============================] - 0s 40us/step\n",
      "....Done\n",
      "\n",
      "Houston,  Artifical Neural Network (ANN) Confusion Matrix et Classification Report  ... \n",
      "\n",
      "Confusion Matrix:\n",
      "[[2240  310]\n",
      " [ 318  853]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.88      0.88      2550\n",
      "         1.0       0.73      0.73      0.73      1171\n",
      "\n",
      "    accuracy                           0.83      3721\n",
      "   macro avg       0.80      0.80      0.80      3721\n",
      "weighted avg       0.83      0.83      0.83      3721\n",
      "\n",
      "....Done\n",
      "\n",
      "Austin,  Artifical Neural Network (ANN) train ... \n",
      "\n",
      "Train on 10966 samples, validate on 2742 samples\n",
      "Epoch 1/10\n",
      "10966/10966 [==============================] - 5s 436us/step - loss: 0.3864 - accuracy: 0.8392 - val_loss: 0.3755 - val_accuracy: 0.8330\n",
      "Epoch 2/10\n",
      "10966/10966 [==============================] - 5s 417us/step - loss: 0.3646 - accuracy: 0.8476 - val_loss: 0.3657 - val_accuracy: 0.8472\n",
      "Epoch 3/10\n",
      "10966/10966 [==============================] - 5s 418us/step - loss: 0.3623 - accuracy: 0.8477 - val_loss: 0.3616 - val_accuracy: 0.8497\n",
      "Epoch 4/10\n",
      "10966/10966 [==============================] - 5s 426us/step - loss: 0.3561 - accuracy: 0.8510 - val_loss: 0.3570 - val_accuracy: 0.8534\n",
      "Epoch 5/10\n",
      "10966/10966 [==============================] - 5s 413us/step - loss: 0.3532 - accuracy: 0.8519 - val_loss: 0.3627 - val_accuracy: 0.8461\n",
      "Epoch 6/10\n",
      "10966/10966 [==============================] - 4s 410us/step - loss: 0.3496 - accuracy: 0.8513 - val_loss: 0.3610 - val_accuracy: 0.8468\n",
      "Epoch 7/10\n",
      "10966/10966 [==============================] - 5s 412us/step - loss: 0.3488 - accuracy: 0.8531 - val_loss: 0.3598 - val_accuracy: 0.8461\n",
      "Epoch 8/10\n",
      "10966/10966 [==============================] - 5s 414us/step - loss: 0.3452 - accuracy: 0.8536 - val_loss: 0.3574 - val_accuracy: 0.8454\n",
      "Epoch 9/10\n",
      "10966/10966 [==============================] - 5s 422us/step - loss: 0.3428 - accuracy: 0.8551 - val_loss: 0.3523 - val_accuracy: 0.8457\n",
      "Epoch 10/10\n",
      "10966/10966 [==============================] - 5s 414us/step - loss: 0.3415 - accuracy: 0.8554 - val_loss: 0.3593 - val_accuracy: 0.8428\n",
      "....Done\n",
      "\n",
      "Austin,  Artifical Neural Network (ANN) Score test  ... \n",
      "3428/3428 [==============================] - 0s 41us/step\n",
      "....Done\n",
      "\n",
      "Austin,  Artifical Neural Network (ANN) Confusion Matrix et Classification Report  ... \n",
      "\n",
      "Confusion Matrix:\n",
      "[[2302  273]\n",
      " [ 233  620]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.89      0.90      2575\n",
      "         1.0       0.69      0.73      0.71       853\n",
      "\n",
      "    accuracy                           0.85      3428\n",
      "   macro avg       0.80      0.81      0.81      3428\n",
      "weighted avg       0.85      0.85      0.85      3428\n",
      "\n",
      "....Done\n",
      "\n",
      "Dallas,  Artifical Neural Network (ANN) train ... \n",
      "\n",
      "Train on 10368 samples, validate on 2592 samples\n",
      "Epoch 1/10\n",
      "10368/10368 [==============================] - 5s 447us/step - loss: 0.4021 - accuracy: 0.8200 - val_loss: 0.3864 - val_accuracy: 0.8295\n",
      "Epoch 2/10\n",
      "10368/10368 [==============================] - 4s 425us/step - loss: 0.3800 - accuracy: 0.8274 - val_loss: 0.3913 - val_accuracy: 0.8291\n",
      "Epoch 3/10\n",
      "10368/10368 [==============================] - 4s 427us/step - loss: 0.3734 - accuracy: 0.8299 - val_loss: 0.3642 - val_accuracy: 0.8341\n",
      "Epoch 4/10\n",
      "10368/10368 [==============================] - 4s 428us/step - loss: 0.3664 - accuracy: 0.8284 - val_loss: 0.3596 - val_accuracy: 0.8337\n",
      "Epoch 5/10\n",
      "10368/10368 [==============================] - 4s 427us/step - loss: 0.3602 - accuracy: 0.8324 - val_loss: 0.3792 - val_accuracy: 0.8252\n",
      "Epoch 6/10\n",
      "10368/10368 [==============================] - 4s 424us/step - loss: 0.3568 - accuracy: 0.8329 - val_loss: 0.3636 - val_accuracy: 0.8387\n",
      "Epoch 7/10\n",
      "10368/10368 [==============================] - 4s 423us/step - loss: 0.3531 - accuracy: 0.8351 - val_loss: 0.3564 - val_accuracy: 0.8430\n",
      "Epoch 8/10\n",
      "10368/10368 [==============================] - 4s 428us/step - loss: 0.3521 - accuracy: 0.8358 - val_loss: 0.3631 - val_accuracy: 0.8391\n",
      "Epoch 9/10\n",
      "10368/10368 [==============================] - 5s 445us/step - loss: 0.3483 - accuracy: 0.8370 - val_loss: 0.3706 - val_accuracy: 0.8380\n",
      "Epoch 10/10\n",
      "10368/10368 [==============================] - 4s 428us/step - loss: 0.3462 - accuracy: 0.8370 - val_loss: 0.3555 - val_accuracy: 0.8383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Done\n",
      "\n",
      "Dallas,  Artifical Neural Network (ANN) Score test  ... \n",
      "3240/3240 [==============================] - 0s 36us/step\n",
      "....Done\n",
      "\n",
      "Dallas,  Artifical Neural Network (ANN) Confusion Matrix et Classification Report  ... \n",
      "\n",
      "Confusion Matrix:\n",
      "[[2355  212]\n",
      " [ 340  333]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.92      0.90      2567\n",
      "         1.0       0.61      0.49      0.55       673\n",
      "\n",
      "    accuracy                           0.83      3240\n",
      "   macro avg       0.74      0.71      0.72      3240\n",
      "weighted avg       0.82      0.83      0.82      3240\n",
      "\n",
      "....Done\n",
      "\n",
      "Charlotte,  Artifical Neural Network (ANN) train ... \n",
      "\n",
      "Train on 11635 samples, validate on 2909 samples\n",
      "Epoch 1/10\n",
      "11635/11635 [==============================] - 4s 369us/step - loss: 0.4262 - accuracy: 0.8041 - val_loss: 0.3940 - val_accuracy: 0.8154\n",
      "Epoch 2/10\n",
      "11635/11635 [==============================] - 4s 349us/step - loss: 0.4048 - accuracy: 0.8166 - val_loss: 0.3942 - val_accuracy: 0.8178\n",
      "Epoch 3/10\n",
      "11635/11635 [==============================] - 4s 352us/step - loss: 0.3984 - accuracy: 0.8211 - val_loss: 0.3901 - val_accuracy: 0.8216\n",
      "Epoch 4/10\n",
      "11635/11635 [==============================] - 4s 362us/step - loss: 0.3914 - accuracy: 0.8240 - val_loss: 0.3850 - val_accuracy: 0.8171\n",
      "Epoch 5/10\n",
      "11635/11635 [==============================] - 4s 344us/step - loss: 0.3888 - accuracy: 0.8237 - val_loss: 0.3846 - val_accuracy: 0.8219\n",
      "Epoch 6/10\n",
      "11635/11635 [==============================] - 4s 348us/step - loss: 0.3825 - accuracy: 0.8295 - val_loss: 0.3831 - val_accuracy: 0.8281\n",
      "Epoch 7/10\n",
      "11635/11635 [==============================] - 4s 342us/step - loss: 0.3813 - accuracy: 0.8256 - val_loss: 0.3767 - val_accuracy: 0.8233\n",
      "Epoch 8/10\n",
      "11635/11635 [==============================] - 4s 345us/step - loss: 0.3785 - accuracy: 0.8252 - val_loss: 0.3750 - val_accuracy: 0.8281\n",
      "Epoch 9/10\n",
      "11635/11635 [==============================] - 4s 347us/step - loss: 0.3744 - accuracy: 0.8294 - val_loss: 0.3729 - val_accuracy: 0.8219\n",
      "Epoch 10/10\n",
      "11635/11635 [==============================] - 4s 345us/step - loss: 0.3723 - accuracy: 0.8292 - val_loss: 0.3713 - val_accuracy: 0.8240\n",
      "....Done\n",
      "\n",
      "Charlotte,  Artifical Neural Network (ANN) Score test  ... \n",
      "3636/3636 [==============================] - 0s 34us/step\n",
      "....Done\n",
      "\n",
      "Charlotte,  Artifical Neural Network (ANN) Confusion Matrix et Classification Report  ... \n",
      "\n",
      "Confusion Matrix:\n",
      "[[2273  304]\n",
      " [ 339  720]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.88      0.88      2577\n",
      "         1.0       0.70      0.68      0.69      1059\n",
      "\n",
      "    accuracy                           0.82      3636\n",
      "   macro avg       0.79      0.78      0.78      3636\n",
      "weighted avg       0.82      0.82      0.82      3636\n",
      "\n",
      "....Done\n",
      "\n",
      "Atlanta,  Artifical Neural Network (ANN) train ... \n",
      "\n",
      "Train on 9960 samples, validate on 2491 samples\n",
      "Epoch 1/10\n",
      "9960/9960 [==============================] - 4s 362us/step - loss: 0.3305 - accuracy: 0.8622 - val_loss: 0.2952 - val_accuracy: 0.8928\n",
      "Epoch 2/10\n",
      "9960/9960 [==============================] - 3s 343us/step - loss: 0.3130 - accuracy: 0.8700 - val_loss: 0.2934 - val_accuracy: 0.8948\n",
      "Epoch 3/10\n",
      "9960/9960 [==============================] - 3s 345us/step - loss: 0.3063 - accuracy: 0.8740 - val_loss: 0.2801 - val_accuracy: 0.8900\n",
      "Epoch 4/10\n",
      "9960/9960 [==============================] - 3s 345us/step - loss: 0.3040 - accuracy: 0.8738 - val_loss: 0.2864 - val_accuracy: 0.8912\n",
      "Epoch 5/10\n",
      "9960/9960 [==============================] - 3s 342us/step - loss: 0.3017 - accuracy: 0.8734 - val_loss: 0.2809 - val_accuracy: 0.8948\n",
      "Epoch 6/10\n",
      "9960/9960 [==============================] - 3s 345us/step - loss: 0.2987 - accuracy: 0.8743 - val_loss: 0.2817 - val_accuracy: 0.8908\n",
      "Epoch 7/10\n",
      "9960/9960 [==============================] - 3s 345us/step - loss: 0.2969 - accuracy: 0.8746 - val_loss: 0.2803 - val_accuracy: 0.8960\n",
      "Epoch 8/10\n",
      "9960/9960 [==============================] - 3s 341us/step - loss: 0.2955 - accuracy: 0.8747 - val_loss: 0.2724 - val_accuracy: 0.8956\n",
      "Epoch 9/10\n",
      "9960/9960 [==============================] - 3s 351us/step - loss: 0.2923 - accuracy: 0.8775 - val_loss: 0.2837 - val_accuracy: 0.8940\n",
      "Epoch 10/10\n",
      "9960/9960 [==============================] - 4s 354us/step - loss: 0.2929 - accuracy: 0.8770 - val_loss: 0.2810 - val_accuracy: 0.8952\n",
      "....Done\n",
      "\n",
      "Atlanta,  Artifical Neural Network (ANN) Score test  ... \n",
      "3113/3113 [==============================] - 0s 36us/step\n",
      "....Done\n",
      "\n",
      "Atlanta,  Artifical Neural Network (ANN) Confusion Matrix et Classification Report  ... \n",
      "\n",
      "Confusion Matrix:\n",
      "[[2438  149]\n",
      " [ 217  309]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.94      0.93      2587\n",
      "         1.0       0.67      0.59      0.63       526\n",
      "\n",
      "    accuracy                           0.88      3113\n",
      "   macro avg       0.80      0.76      0.78      3113\n",
      "weighted avg       0.88      0.88      0.88      3113\n",
      "\n",
      "....Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for city in cities :\n",
    "    model_ann(\"data/clean_twpoi_data/{}.csv\".format(city), city)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrafficWeatherEvent_June18_Aug18_Publish,  Artifical Neural Network (ANN) train ... \n",
      "\n",
      "Train on 68007 samples, validate on 17002 samples\n",
      "Epoch 1/10\n",
      "68007/68007 [==============================] - 24s 360us/step - loss: 0.4289 - accuracy: 0.8003 - val_loss: 0.4207 - val_accuracy: 0.8052\n",
      "Epoch 2/10\n",
      "68007/68007 [==============================] - 25s 362us/step - loss: 0.4109 - accuracy: 0.8127 - val_loss: 0.4117 - val_accuracy: 0.8076\n",
      "Epoch 3/10\n",
      "19190/68007 [=======>......................] - ETA: 24s - loss: 0.4033 - accuracy: 0.8177"
     ]
    }
   ],
   "source": [
    "for city in c :\n",
    "    model_ann(\"data/clean_twpoi_data/{}.csv\".format(city), city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
